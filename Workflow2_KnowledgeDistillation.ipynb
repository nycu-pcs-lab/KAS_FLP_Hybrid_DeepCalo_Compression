{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ccb3c3-372e-4db7-a0f9-518deccf2963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU ID:1,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run /Desktop/Share/CUDA_DEVICE_setup.py -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a296cea-5b34-4879-bfa8-85de6294e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"  # set as the using GPU ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86e28d6f-ef3b-4358-bf0d-b0b916df2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2786247-d496-4eb3-a928-68965114bf0b",
   "metadata": {},
   "source": [
    "# Build the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97daa74c-3c45-4cc5-b219-55ac6054448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCaloDataset(Dataset):\n",
    "    def __init__(self, data_path, target_path, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with data and target paths.\n",
    "        \n",
    "        Parameters:\n",
    "        data_path (str): Path to the .npy file containing image data.\n",
    "        target_path (str): Path to the .npy file containing target data.\n",
    "        transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = np.load(data_path).astype(np.float32)\n",
    "        self.targets = np.load(target_path).astype(np.float32)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        # Permute the sample to have the channel dimension first\n",
    "        sample = np.transpose(sample, (2, 0, 1))  # From [56, 11, 4] to [4, 56, 11]\n",
    "\n",
    "        target = np.expand_dims(target, -1)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e556ba43-7769-4c7f-9aad-95f8fc08976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define file paths\n",
    "base_path = '/Desktop/CodeFolder/PCS/dataset/DeepCalo100K'\n",
    "#base_path = '.'\n",
    "train_data_path = f'{base_path}/dcalo_img_train.npy'\n",
    "train_target_path = f'{base_path}/dcalo_img_train_target.npy'\n",
    "val_data_path = f'{base_path}/dcalo_img_val.npy'\n",
    "val_target_path = f'{base_path}/dcalo_img_val_target.npy'\n",
    "test_data_path = f'{base_path}/dcalo_img_test.npy'\n",
    "test_target_path = f'{base_path}/dcalo_img_test_target.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22f0a877-86bd-4b7f-93c1-fff6134881b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = DeepCaloDataset(train_data_path, train_target_path)\n",
    "val_dataset = DeepCaloDataset(val_data_path, val_target_path)\n",
    "test_dataset = DeepCaloDataset(test_data_path, test_target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa4eed4f-d24e-40f0-9eff-b757fca61619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "tr_batch_size = 64\n",
    "test_batch_size = 16\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=tr_batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=test_batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d74ab7-5b3f-49b3-8e93-e88bc3c05e2f",
   "metadata": {},
   "source": [
    "## Have a look at images for DeepCalo\n",
    "Each image has a shape of (56, 11, 4), 4 channel currosponding to 4 layers if electromegnatic calorememter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f06f538-9601-46f9-ad70-86cfc1fba775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6550099f-3981-4bed-b314-4a993ef3e30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 56, 11])\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the first batch \n",
    "data_iter = iter(test_loader)\n",
    "batch = next(data_iter)\n",
    "\n",
    "# Extract samples and targets\n",
    "# Show the first 2 data\n",
    "samples, targets = batch\n",
    "samples, targets = samples[:2], targets[:2]\n",
    "\n",
    "print(samples.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26dc9b65-6b8a-4375-8f2e-d234bc7ba9bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First image: \n",
      "Target: 146.55\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seocnd image: \n",
      "Target: 101.04\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"First image: \")\n",
    "print(f\"Target: {targets[0].item():.2f}\")\n",
    "\n",
    "# Plot each channel separately\n",
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.imshow(samples[0][i])\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Channel {i + 1}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Seocnd image: \")\n",
    "print(f\"Target: {targets[1].item():.2f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.imshow(samples[1][i])\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Channel {i + 1}\")\n",
    "plt.show()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714962ed-5e60-49a5-8751-b9fe7fc4ae15",
   "metadata": {},
   "source": [
    "# DeepCalo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "963c4400-1285-4bd3-84d7-50038edacfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eae85c70-1fee-4d37-9aec-6c930622f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eac5afad-c110-4e78-9187-40082a68ed1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c88a0e0-0312-455e-8ff5-3bd9089337cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import brevitas.nn as qnn\n",
    "\n",
    "class DeepCaloModelFull(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCaloModelFull, self).__init__()\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=(1, 5), mode='nearest')\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.block1_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block1_conv1 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(32)\n",
    "        self.block1_conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.block2_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block2_conv1 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(64)\n",
    "        self.block2_conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.block3_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block3_conv1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(128)\n",
    "        self.block3_conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.block4_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block4_conv1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4_1 = nn.BatchNorm2d(256)\n",
    "        self.block4_conv2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn4_2 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        x = self.block1_pool(x)\n",
    "        x = F.relu(self.bn1_1(self.block1_conv1(x)))\n",
    "        x = F.relu(self.bn1_2(self.block1_conv2(x)))\n",
    "\n",
    "        x = self.block2_pool(x)\n",
    "        x = F.relu(self.bn2_1(self.block2_conv1(x)))\n",
    "        x = F.relu(self.bn2_2(self.block2_conv2(x)))\n",
    "\n",
    "        x = self.block3_pool(x)\n",
    "        x = F.relu(self.bn3_1(self.block3_conv1(x)))\n",
    "        x = F.relu(self.bn3_2(self.block3_conv2(x)))\n",
    "\n",
    "        x = self.block4_pool(x)\n",
    "        x = F.relu(self.bn4_1(self.block4_conv1(x)))\n",
    "        x = F.relu(self.bn4_2(self.block4_conv2(x)))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42ff295d-c1c4-45fd-949e-53559304e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_calo_model = DeepCaloModelFull().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "512b3a11-8fc0-41fd-87c8-3dd5530a99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0aa9c8dc-5c0f-4602-904d-ad43f6069b4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "DeepCaloModelFull                        --\n",
       "├─Upsample: 1-1                          --\n",
       "├─Conv2d: 1-2                            1,616\n",
       "├─BatchNorm2d: 1-3                       32\n",
       "├─MaxPool2d: 1-4                         --\n",
       "├─Conv2d: 1-5                            4,640\n",
       "├─BatchNorm2d: 1-6                       64\n",
       "├─Conv2d: 1-7                            9,248\n",
       "├─BatchNorm2d: 1-8                       64\n",
       "├─MaxPool2d: 1-9                         --\n",
       "├─Conv2d: 1-10                           18,496\n",
       "├─BatchNorm2d: 1-11                      128\n",
       "├─Conv2d: 1-12                           36,928\n",
       "├─BatchNorm2d: 1-13                      128\n",
       "├─MaxPool2d: 1-14                        --\n",
       "├─Conv2d: 1-15                           73,856\n",
       "├─BatchNorm2d: 1-16                      256\n",
       "├─Conv2d: 1-17                           147,584\n",
       "├─BatchNorm2d: 1-18                      256\n",
       "├─MaxPool2d: 1-19                        --\n",
       "├─Conv2d: 1-20                           295,168\n",
       "├─BatchNorm2d: 1-21                      512\n",
       "├─Conv2d: 1-22                           590,080\n",
       "├─BatchNorm2d: 1-23                      512\n",
       "├─Linear: 1-24                           590,080\n",
       "├─Linear: 1-25                           65,792\n",
       "├─Linear: 1-26                           257\n",
       "=================================================================\n",
       "Total params: 1,835,697\n",
       "Trainable params: 1,835,697\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(deep_calo_model, input_shape = [1, 56, 11, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77ebe6fa-71f8-48e1-9bba-b8e8ab39f0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepCaloModelFull(\n",
       "  (upsample): Upsample(scale_factor=(1.0, 5.0), mode=nearest)\n",
       "  (conv1): Conv2d(4, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block1_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block1_conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block1_conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block2_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block2_conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block2_conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block3_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block3_conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block3_conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block4_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block4_conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn4_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block4_conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn4_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=2304, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_calo_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5ba58-e014-4074-9157-71566b6112f5",
   "metadata": {},
   "source": [
    "# Train the initial model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7fe38e9-0e36-41eb-81c4-fb1463e36d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return torch.mean(torch.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad4c8f2a-7a40-4899-838e-92d3166612f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = '/Desktop/CodeFolder/PCS/Group1/' + 'best_deepalo_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6f590a3-0f39-4c6c-bdd3-099d44ca4054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(deep_calo_model, train_loader, test_loader, num_epochs=10, lr=0.001, best_model_path = 'best_deepalo_model.pth'):\n",
    "    # Define the loss function and the optimizer\n",
    "    criterion = nn.L1Loss()  # Mean Absolute Error (MAE) Loss\n",
    "    optimizer = optim.Adam(deep_calo_model.parameters(), lr=lr)\n",
    "\n",
    "    train_accuracies = []\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    test_losses = []\n",
    "\n",
    "    best_loss = float('inf')  # Initialize the best loss to a high value\n",
    "      # Path to save the best model\n",
    "\n",
    "    print(\"Start Training:\")\n",
    "    for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
    "        running_loss = 0.0  # Initialize the running loss for each epoch\n",
    "        total_mae = 0.0  # Initialize the running MAE for each epoch\n",
    "        total = 0\n",
    "\n",
    "        deep_calo_model.train()  # Set the network to training mode\n",
    "\n",
    "        for data in train_loader:\n",
    "            # Get the inputs and labels from the data loader\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Move the inputs and labels to the specified device (CPU or GPU)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients to avoid accumulation from previous iterations\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform the forward pass: compute the network's outputs\n",
    "            outputs = deep_calo_model(inputs)\n",
    "\n",
    "            # Compute the loss using the criterion\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Perform the backward pass: compute the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the network parameters using the optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for the current mini-batch\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate MAE for the current mini-batch\n",
    "            total_mae += mean_absolute_error(outputs.cpu(), labels.cpu()) * inputs.size(0)\n",
    "            total += inputs.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_mae = total_mae / total\n",
    "        train_accuracies.append(train_mae)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        deep_calo_model.eval()  # Set the network to evaluation mode\n",
    "        test_mae = 0.0\n",
    "        test_loss = 0.0\n",
    "        total_test = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = deep_calo_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                test_mae += mean_absolute_error(outputs.cpu(), labels.cpu()) * inputs.size(0)\n",
    "                total_test += inputs.size(0)\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_mae /= total_test\n",
    "        test_accuracies.append(test_mae)\n",
    "        test_losses.append(test_loss)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "              f'Training Loss: {train_loss:.4f}, Training MAE: {train_mae:.4f}, '\n",
    "              f'Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}')\n",
    "\n",
    "        # Check if this is the best model so far\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            torch.save(deep_calo_model.state_dict(), best_model_path)\n",
    "            print(f'Saved model with test loss: {best_loss:.4f}')\n",
    "\n",
    "    print('Finished Training')\n",
    "    return train_accuracies, train_losses, test_accuracies, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "974df5f5-fe3d-478e-9845-ecff19cde8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model performance on the test data for first 5 batches\n",
    "def test_model(model, test_loader, device, num_batches=5):\n",
    "\n",
    "    criterion = nn.L1Loss()  # Mean Absolute Error (MAE) Loss\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_mae = 0.0\n",
    "    test_loss = 0.0\n",
    "    total_test = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            if batch_count >= num_batches:\n",
    "                break\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            test_mae += mean_absolute_error(outputs.cpu(), labels.cpu()) * inputs.size(0)\n",
    "            total_test += inputs.size(0)\n",
    "            batch_count += 1\n",
    "\n",
    "            print(f'Batch {batch_count}, Test MAE: {mean_absolute_error(outputs.cpu(), labels.cpu()):.4f}, Test Loss: {loss.item():.4f}')\n",
    "\n",
    "    test_loss /= batch_count\n",
    "    test_mae /= total_test\n",
    "    print(f'Average Test MAE for first {num_batches} batches: {test_mae:.4f}')\n",
    "    print(f'Average Test Loss for first {num_batches} batches: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ca80832-a196-4b40-911a-eb602e243115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained DeepCalo:  /Desktop/CodeFolder/PCS/Group1/best_deepalo_model.pth\n",
      "Batch 1, Test MAE: 7.8209, Test Loss: 7.8209\n",
      "Batch 2, Test MAE: 4.6491, Test Loss: 4.6491\n",
      "Batch 3, Test MAE: 8.6994, Test Loss: 8.6994\n",
      "Batch 4, Test MAE: 4.3375, Test Loss: 4.3375\n",
      "Batch 5, Test MAE: 3.7579, Test Loss: 3.7579\n",
      "Average Test MAE for first 5 batches: 5.8530\n",
      "Average Test Loss for first 5 batches: 5.8530\n"
     ]
    }
   ],
   "source": [
    "# train 第一遍\n",
    "\n",
    "load_pretrain = True\n",
    "if load_pretrain:\n",
    "    print(\"Load pretrained DeepCalo: \", best_model_path)\n",
    "    deep_calo_model.load_state_dict(torch.load(best_model_path))\n",
    "    test_model(deep_calo_model, test_loader, device, num_batches=5)\n",
    "else:\n",
    "    train_accuracies, train_losses, test_accuracies, test_losses = \\\n",
    "    train_model(deep_calo_model, train_loader, test_loader, num_epochs=100, lr = 0.001, best_model_path = best_model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06685d5b-0793-4682-b512-17184bd56153",
   "metadata": {},
   "source": [
    "# Student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4241c55-58b8-479f-b252-1462a61826a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCaloStudent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCaloStudent, self).__init__()\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=(1, 5), mode='nearest')\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4, 8, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        \n",
    "        self.block1_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block1_conv1 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(16)\n",
    "        self.block1_conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.block2_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block2_conv1 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(32)\n",
    "        self.block2_conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.block3_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block3_conv1 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(64)\n",
    "        self.block3_conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.block4_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block4_conv1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn4_1 = nn.BatchNorm2d(128)\n",
    "        self.block4_conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn4_2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        x = self.block1_pool(x)\n",
    "        x = F.relu(self.bn1_1(self.block1_conv1(x)))\n",
    "        x = F.relu(self.bn1_2(self.block1_conv2(x)))\n",
    "\n",
    "        x = self.block2_pool(x)\n",
    "        x = F.relu(self.bn2_1(self.block2_conv1(x)))\n",
    "        x = F.relu(self.bn2_2(self.block2_conv2(x)))\n",
    "\n",
    "        x = self.block3_pool(x)\n",
    "        x = F.relu(self.bn3_1(self.block3_conv1(x)))\n",
    "        x = F.relu(self.bn3_2(self.block3_conv2(x)))\n",
    "\n",
    "        x = self.block4_pool(x)\n",
    "        x = F.relu(self.bn4_1(self.block4_conv1(x)))\n",
    "        x = F.relu(self.bn4_2(self.block4_conv2(x)))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c45d47a3-43f2-4066-af75-877ff9133c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_calo_student = DeepCaloStudent().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a102c53d-f4c9-4cb0-ab27-3b8297ef86bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "DeepCaloStudent                          --\n",
       "├─Upsample: 1-1                          --\n",
       "├─Conv2d: 1-2                            808\n",
       "├─BatchNorm2d: 1-3                       16\n",
       "├─MaxPool2d: 1-4                         --\n",
       "├─Conv2d: 1-5                            1,168\n",
       "├─BatchNorm2d: 1-6                       32\n",
       "├─Conv2d: 1-7                            2,320\n",
       "├─BatchNorm2d: 1-8                       32\n",
       "├─MaxPool2d: 1-9                         --\n",
       "├─Conv2d: 1-10                           4,640\n",
       "├─BatchNorm2d: 1-11                      64\n",
       "├─Conv2d: 1-12                           9,248\n",
       "├─BatchNorm2d: 1-13                      64\n",
       "├─MaxPool2d: 1-14                        --\n",
       "├─Conv2d: 1-15                           18,496\n",
       "├─BatchNorm2d: 1-16                      128\n",
       "├─Conv2d: 1-17                           36,928\n",
       "├─BatchNorm2d: 1-18                      128\n",
       "├─MaxPool2d: 1-19                        --\n",
       "├─Conv2d: 1-20                           73,856\n",
       "├─BatchNorm2d: 1-21                      256\n",
       "├─Conv2d: 1-22                           147,584\n",
       "├─BatchNorm2d: 1-23                      256\n",
       "├─Linear: 1-24                           295,168\n",
       "├─Linear: 1-25                           65,792\n",
       "├─Linear: 1-26                           257\n",
       "=================================================================\n",
       "Total params: 657,241\n",
       "Trainable params: 657,241\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(deep_calo_student, input_shape = [1, 56, 11, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a5295d-f09d-40e0-ba48-380e66b53635",
   "metadata": {},
   "source": [
    "## train the student model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48104e33-eacb-4a3b-8f75-188e563d023a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training:\n",
      "Epoch [1/200], Training Loss: 13.3909, Training MAE: 13.3904, Test Loss: 19.3906, Test MAE: 19.3906\n",
      "Saved model with test loss: 19.3906\n",
      "Epoch [2/200], Training Loss: 11.1727, Training MAE: 11.1728, Test Loss: 6.3587, Test MAE: 6.3587\n",
      "Saved model with test loss: 6.3587\n",
      "Epoch [3/200], Training Loss: 10.9426, Training MAE: 10.9408, Test Loss: 24.0635, Test MAE: 24.0635\n",
      "Epoch [4/200], Training Loss: 10.7788, Training MAE: 10.7784, Test Loss: 13.7652, Test MAE: 13.7652\n",
      "Epoch [5/200], Training Loss: 10.2792, Training MAE: 10.2785, Test Loss: 7.0069, Test MAE: 7.0069\n",
      "Epoch [6/200], Training Loss: 10.3886, Training MAE: 10.3820, Test Loss: 5.4937, Test MAE: 5.4937\n",
      "Saved model with test loss: 5.4937\n",
      "Epoch [7/200], Training Loss: 10.5370, Training MAE: 10.5368, Test Loss: 7.0407, Test MAE: 7.0407\n",
      "Epoch [8/200], Training Loss: 10.1099, Training MAE: 10.1117, Test Loss: 16.5612, Test MAE: 16.5612\n",
      "Epoch [9/200], Training Loss: 9.7789, Training MAE: 9.7808, Test Loss: 10.0970, Test MAE: 10.0970\n",
      "Epoch [10/200], Training Loss: 9.8203, Training MAE: 9.8205, Test Loss: 7.1037, Test MAE: 7.1037\n",
      "Epoch [11/200], Training Loss: 9.9673, Training MAE: 9.9695, Test Loss: 6.9057, Test MAE: 6.9057\n",
      "Epoch [12/200], Training Loss: 9.8940, Training MAE: 9.8940, Test Loss: 12.1146, Test MAE: 12.1146\n",
      "Epoch [13/200], Training Loss: 9.6172, Training MAE: 9.6180, Test Loss: 17.6153, Test MAE: 17.6153\n",
      "Epoch [14/200], Training Loss: 9.5671, Training MAE: 9.5661, Test Loss: 6.3567, Test MAE: 6.3567\n",
      "Epoch [15/200], Training Loss: 9.1651, Training MAE: 9.1620, Test Loss: 16.3881, Test MAE: 16.3881\n",
      "Epoch [16/200], Training Loss: 9.6547, Training MAE: 9.6562, Test Loss: 5.7295, Test MAE: 5.7295\n",
      "Epoch [17/200], Training Loss: 9.7603, Training MAE: 9.7588, Test Loss: 5.5997, Test MAE: 5.5997\n",
      "Epoch [18/200], Training Loss: 9.3317, Training MAE: 9.3294, Test Loss: 7.0149, Test MAE: 7.0149\n",
      "Epoch [19/200], Training Loss: 9.5290, Training MAE: 9.5311, Test Loss: 7.4604, Test MAE: 7.4604\n",
      "Epoch [20/200], Training Loss: 9.5423, Training MAE: 9.5421, Test Loss: 6.2314, Test MAE: 6.2314\n",
      "Epoch [21/200], Training Loss: 9.3029, Training MAE: 9.3024, Test Loss: 8.9742, Test MAE: 8.9742\n",
      "Epoch [22/200], Training Loss: 9.3999, Training MAE: 9.4013, Test Loss: 6.1147, Test MAE: 6.1147\n",
      "Epoch [23/200], Training Loss: 9.3234, Training MAE: 9.3179, Test Loss: 9.5779, Test MAE: 9.5779\n",
      "Epoch [24/200], Training Loss: 9.1833, Training MAE: 9.1795, Test Loss: 6.7520, Test MAE: 6.7520\n",
      "Epoch [25/200], Training Loss: 9.1160, Training MAE: 9.1145, Test Loss: 5.2147, Test MAE: 5.2147\n",
      "Saved model with test loss: 5.2147\n",
      "Epoch [26/200], Training Loss: 9.2432, Training MAE: 9.2458, Test Loss: 6.0870, Test MAE: 6.0870\n",
      "Epoch [27/200], Training Loss: 9.3540, Training MAE: 9.3511, Test Loss: 5.7816, Test MAE: 5.7816\n",
      "Epoch [28/200], Training Loss: 9.1399, Training MAE: 9.1410, Test Loss: 5.2575, Test MAE: 5.2576\n",
      "Epoch [29/200], Training Loss: 8.9996, Training MAE: 8.9993, Test Loss: 5.3919, Test MAE: 5.3919\n",
      "Epoch [30/200], Training Loss: 8.9749, Training MAE: 8.9750, Test Loss: 5.3264, Test MAE: 5.3264\n",
      "Epoch [31/200], Training Loss: 9.1724, Training MAE: 9.1728, Test Loss: 6.7004, Test MAE: 6.7004\n",
      "Epoch [32/200], Training Loss: 8.7476, Training MAE: 8.7443, Test Loss: 12.2167, Test MAE: 12.2167\n",
      "Epoch [33/200], Training Loss: 8.8607, Training MAE: 8.8603, Test Loss: 7.2616, Test MAE: 7.2616\n",
      "Epoch [34/200], Training Loss: 8.4648, Training MAE: 8.4578, Test Loss: 10.8313, Test MAE: 10.8313\n",
      "Epoch [35/200], Training Loss: 8.6354, Training MAE: 8.6352, Test Loss: 7.8489, Test MAE: 7.8489\n",
      "Epoch [36/200], Training Loss: 8.4315, Training MAE: 8.4306, Test Loss: 5.3151, Test MAE: 5.3151\n",
      "Epoch [37/200], Training Loss: 8.3723, Training MAE: 8.3726, Test Loss: 6.5926, Test MAE: 6.5926\n",
      "Epoch [38/200], Training Loss: 8.5060, Training MAE: 8.5080, Test Loss: 5.9036, Test MAE: 5.9036\n",
      "Epoch [39/200], Training Loss: 8.2455, Training MAE: 8.2445, Test Loss: 7.9456, Test MAE: 7.9456\n",
      "Epoch [40/200], Training Loss: 8.1883, Training MAE: 8.1891, Test Loss: 5.5898, Test MAE: 5.5898\n",
      "Epoch [41/200], Training Loss: 8.1699, Training MAE: 8.1716, Test Loss: 10.7018, Test MAE: 10.7018\n",
      "Epoch [42/200], Training Loss: 8.3010, Training MAE: 8.3012, Test Loss: 7.4468, Test MAE: 7.4468\n",
      "Epoch [43/200], Training Loss: 8.1287, Training MAE: 8.1265, Test Loss: 6.4743, Test MAE: 6.4743\n",
      "Epoch [44/200], Training Loss: 8.0626, Training MAE: 8.0635, Test Loss: 5.2837, Test MAE: 5.2837\n",
      "Epoch [45/200], Training Loss: 8.0302, Training MAE: 8.0306, Test Loss: 5.3955, Test MAE: 5.3955\n",
      "Epoch [46/200], Training Loss: 8.0460, Training MAE: 8.0456, Test Loss: 5.3390, Test MAE: 5.3390\n",
      "Epoch [47/200], Training Loss: 8.1141, Training MAE: 8.1154, Test Loss: 6.7815, Test MAE: 6.7815\n",
      "Epoch [48/200], Training Loss: 7.8147, Training MAE: 7.8157, Test Loss: 6.3025, Test MAE: 6.3025\n",
      "Epoch [49/200], Training Loss: 7.9971, Training MAE: 7.9967, Test Loss: 5.1320, Test MAE: 5.1320\n",
      "Saved model with test loss: 5.1320\n",
      "Epoch [50/200], Training Loss: 7.9238, Training MAE: 7.9179, Test Loss: 5.4215, Test MAE: 5.4215\n",
      "Epoch [51/200], Training Loss: 7.7650, Training MAE: 7.7642, Test Loss: 5.3783, Test MAE: 5.3783\n",
      "Epoch [52/200], Training Loss: 7.8715, Training MAE: 7.8713, Test Loss: 5.8169, Test MAE: 5.8169\n",
      "Epoch [53/200], Training Loss: 7.7969, Training MAE: 7.7984, Test Loss: 5.3873, Test MAE: 5.3873\n",
      "Epoch [54/200], Training Loss: 7.7858, Training MAE: 7.7843, Test Loss: 5.9358, Test MAE: 5.9358\n",
      "Epoch [55/200], Training Loss: 7.8616, Training MAE: 7.8625, Test Loss: 5.3661, Test MAE: 5.3661\n",
      "Epoch [56/200], Training Loss: 7.6722, Training MAE: 7.6727, Test Loss: 5.7190, Test MAE: 5.7190\n",
      "Epoch [57/200], Training Loss: 8.0606, Training MAE: 8.0614, Test Loss: 6.1545, Test MAE: 6.1545\n",
      "Epoch [58/200], Training Loss: 7.6837, Training MAE: 7.6838, Test Loss: 5.4102, Test MAE: 5.4102\n",
      "Epoch [59/200], Training Loss: 7.7026, Training MAE: 7.7043, Test Loss: 7.0139, Test MAE: 7.0139\n",
      "Epoch [60/200], Training Loss: 7.5536, Training MAE: 7.5544, Test Loss: 6.2600, Test MAE: 6.2600\n",
      "Epoch [61/200], Training Loss: 7.6276, Training MAE: 7.6285, Test Loss: 5.1579, Test MAE: 5.1579\n",
      "Epoch [62/200], Training Loss: 7.6959, Training MAE: 7.6946, Test Loss: 5.1513, Test MAE: 5.1513\n",
      "Epoch [63/200], Training Loss: 7.6164, Training MAE: 7.6178, Test Loss: 6.7244, Test MAE: 6.7244\n",
      "Epoch [64/200], Training Loss: 7.5752, Training MAE: 7.5727, Test Loss: 5.5185, Test MAE: 5.5185\n",
      "Epoch [65/200], Training Loss: 7.5944, Training MAE: 7.5957, Test Loss: 5.4096, Test MAE: 5.4096\n",
      "Epoch [66/200], Training Loss: 7.4592, Training MAE: 7.4592, Test Loss: 5.2507, Test MAE: 5.2507\n",
      "Epoch [67/200], Training Loss: 7.6528, Training MAE: 7.6539, Test Loss: 6.1755, Test MAE: 6.1755\n",
      "Epoch [68/200], Training Loss: 7.6797, Training MAE: 7.6794, Test Loss: 8.6094, Test MAE: 8.6094\n",
      "Epoch [69/200], Training Loss: 7.7530, Training MAE: 7.7542, Test Loss: 7.0706, Test MAE: 7.0706\n",
      "Epoch [70/200], Training Loss: 7.4425, Training MAE: 7.4399, Test Loss: 6.9002, Test MAE: 6.9002\n",
      "Epoch [71/200], Training Loss: 7.5768, Training MAE: 7.5740, Test Loss: 6.7220, Test MAE: 6.7220\n",
      "Epoch [72/200], Training Loss: 7.5125, Training MAE: 7.5119, Test Loss: 5.1006, Test MAE: 5.1006\n",
      "Saved model with test loss: 5.1006\n",
      "Epoch [73/200], Training Loss: 7.5114, Training MAE: 7.5128, Test Loss: 5.1892, Test MAE: 5.1892\n",
      "Epoch [74/200], Training Loss: 7.4226, Training MAE: 7.4229, Test Loss: 5.6826, Test MAE: 5.6826\n",
      "Epoch [75/200], Training Loss: 7.5056, Training MAE: 7.5010, Test Loss: 5.0817, Test MAE: 5.0817\n",
      "Saved model with test loss: 5.0817\n",
      "Epoch [76/200], Training Loss: 7.5399, Training MAE: 7.5402, Test Loss: 5.1578, Test MAE: 5.1578\n",
      "Epoch [77/200], Training Loss: 7.6496, Training MAE: 7.6460, Test Loss: 5.4906, Test MAE: 5.4906\n",
      "Epoch [78/200], Training Loss: 7.3253, Training MAE: 7.3211, Test Loss: 6.0980, Test MAE: 6.0980\n",
      "Epoch [79/200], Training Loss: 7.2921, Training MAE: 7.2907, Test Loss: 8.4676, Test MAE: 8.4676\n",
      "Epoch [80/200], Training Loss: 7.4018, Training MAE: 7.3990, Test Loss: 6.1254, Test MAE: 6.1254\n",
      "Epoch [81/200], Training Loss: 7.4084, Training MAE: 7.4027, Test Loss: 6.0579, Test MAE: 6.0579\n",
      "Epoch [82/200], Training Loss: 7.4282, Training MAE: 7.4274, Test Loss: 5.4137, Test MAE: 5.4137\n",
      "Epoch [83/200], Training Loss: 7.2326, Training MAE: 7.2331, Test Loss: 5.9025, Test MAE: 5.9025\n",
      "Epoch [84/200], Training Loss: 7.2933, Training MAE: 7.2939, Test Loss: 6.1633, Test MAE: 6.1633\n",
      "Epoch [85/200], Training Loss: 7.4346, Training MAE: 7.4351, Test Loss: 5.2464, Test MAE: 5.2464\n",
      "Epoch [86/200], Training Loss: 7.3120, Training MAE: 7.3056, Test Loss: 5.6291, Test MAE: 5.6291\n",
      "Epoch [87/200], Training Loss: 7.4204, Training MAE: 7.4161, Test Loss: 5.4204, Test MAE: 5.4204\n",
      "Epoch [88/200], Training Loss: 7.3824, Training MAE: 7.3808, Test Loss: 7.9925, Test MAE: 7.9925\n",
      "Epoch [89/200], Training Loss: 7.3467, Training MAE: 7.3480, Test Loss: 5.2043, Test MAE: 5.2043\n",
      "Epoch [90/200], Training Loss: 7.1420, Training MAE: 7.1439, Test Loss: 7.2027, Test MAE: 7.2027\n",
      "Epoch [91/200], Training Loss: 7.3576, Training MAE: 7.3539, Test Loss: 6.0837, Test MAE: 6.0837\n",
      "Epoch [92/200], Training Loss: 7.3971, Training MAE: 7.3977, Test Loss: 8.0046, Test MAE: 8.0046\n",
      "Epoch [93/200], Training Loss: 7.2213, Training MAE: 7.2195, Test Loss: 5.8220, Test MAE: 5.8220\n",
      "Epoch [94/200], Training Loss: 7.3183, Training MAE: 7.3127, Test Loss: 5.7694, Test MAE: 5.7694\n",
      "Epoch [95/200], Training Loss: 7.3961, Training MAE: 7.3971, Test Loss: 5.8522, Test MAE: 5.8522\n",
      "Epoch [96/200], Training Loss: 7.3425, Training MAE: 7.3409, Test Loss: 5.7387, Test MAE: 5.7387\n",
      "Epoch [97/200], Training Loss: 7.3163, Training MAE: 7.3152, Test Loss: 5.6222, Test MAE: 5.6222\n",
      "Epoch [98/200], Training Loss: 7.1892, Training MAE: 7.1899, Test Loss: 5.2373, Test MAE: 5.2373\n",
      "Epoch [99/200], Training Loss: 7.2217, Training MAE: 7.2210, Test Loss: 6.3115, Test MAE: 6.3115\n",
      "Epoch [100/200], Training Loss: 7.4490, Training MAE: 7.4504, Test Loss: 6.1556, Test MAE: 6.1556\n",
      "Epoch [101/200], Training Loss: 7.3588, Training MAE: 7.3590, Test Loss: 5.3078, Test MAE: 5.3079\n",
      "Epoch [102/200], Training Loss: 7.3803, Training MAE: 7.3807, Test Loss: 5.1783, Test MAE: 5.1783\n",
      "Epoch [103/200], Training Loss: 7.3874, Training MAE: 7.3864, Test Loss: 9.4180, Test MAE: 9.4180\n",
      "Epoch [104/200], Training Loss: 7.2316, Training MAE: 7.2324, Test Loss: 5.2607, Test MAE: 5.2607\n",
      "Epoch [105/200], Training Loss: 7.2777, Training MAE: 7.2768, Test Loss: 6.5146, Test MAE: 6.5146\n",
      "Epoch [106/200], Training Loss: 7.1986, Training MAE: 7.1990, Test Loss: 12.4717, Test MAE: 12.4717\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_accuracies, train_losses, test_accuracies, test_losses \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep_calo_student\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_deepcalo_student_noKD.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 43\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(deep_calo_model, train_loader, test_loader, num_epochs, lr, best_model_path)\u001b[0m\n\u001b[1;32m     40\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Update the network parameters using the optimizer\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Accumulate the loss for the current mini-batch\u001b[39;00m\n\u001b[1;32m     46\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/USEGPU/lib/python3.8/site-packages/torch/optim/adam.py:95\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m     grad\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m], p\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2, grad, grad)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_accuracies, train_losses, test_accuracies, test_losses = \\\n",
    "    train_model(deep_calo_student, train_loader, test_loader, num_epochs=200, lr = 0.001, best_model_path = 'best_deepcalo_student_noKD.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d9b1b7-c2d3-4598-a65f-1b0476803e49",
   "metadata": {},
   "source": [
    "## Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "507bb4a2-eb69-4b90-9568-700a9539e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the teacher model to return intermediate output of fc2\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=(1, 5), mode='nearest')\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.block1_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block1_conv1 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(32)\n",
    "        self.block1_conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.block2_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block2_conv1 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(64)\n",
    "        self.block2_conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.block3_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block3_conv1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(128)\n",
    "        self.block3_conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.block4_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block4_conv1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4_1 = nn.BatchNorm2d(256)\n",
    "        self.block4_conv2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn4_2 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        x = self.block1_pool(x)\n",
    "        x = F.relu(self.bn1_1(self.block1_conv1(x)))\n",
    "        x = F.relu(self.bn1_2(self.block1_conv2(x)))\n",
    "\n",
    "        x = self.block2_pool(x)\n",
    "        x = F.relu(self.bn2_1(self.block2_conv1(x)))\n",
    "        x = F.relu(self.bn2_2(self.block2_conv2(x)))\n",
    "\n",
    "        x = self.block3_pool(x)\n",
    "        x = F.relu(self.bn3_1(self.block3_conv1(x)))\n",
    "        x = F.relu(self.bn3_2(self.block3_conv2(x)))\n",
    "\n",
    "        x = self.block4_pool(x)\n",
    "        x = F.relu(self.bn4_1(self.block4_conv1(x)))\n",
    "        x = F.relu(self.bn4_2(self.block4_conv2(x)))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        fc2_output = F.relu(self.fc2(x))\n",
    "        x = self.fc3(fc2_output)\n",
    "        x = F.relu(x)\n",
    "        return x, fc2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74065a58-545a-4c49-ac70-1ee8197bca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the student model with halved conv2d filters\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentModel, self).__init__()\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=(1, 5), mode='nearest')\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4, 8, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        \n",
    "        self.block1_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block1_conv1 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(16)\n",
    "        self.block1_conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.block2_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block2_conv1 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(32)\n",
    "        self.block2_conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.block3_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block3_conv1 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(64)\n",
    "        self.block3_conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.block4_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block4_conv1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn4_1 = nn.BatchNorm2d(128)\n",
    "        self.block4_conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn4_2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        x = self.block1_pool(x)\n",
    "        x = F.relu(self.bn1_1(self.block1_conv1(x)))\n",
    "        x = F.relu(self.bn1_2(self.block1_conv2(x)))\n",
    "\n",
    "        x = self.block2_pool(x)\n",
    "        x = F.relu(self.bn2_1(self.block2_conv1(x)))\n",
    "        x = F.relu(self.bn2_2(self.block2_conv2(x)))\n",
    "\n",
    "        x = self.block3_pool(x)\n",
    "        x = F.relu(self.bn3_1(self.block3_conv1(x)))\n",
    "        x = F.relu(self.bn3_2(self.block3_conv2(x)))\n",
    "\n",
    "        x = self.block4_pool(x)\n",
    "        x = F.relu(self.bn4_1(self.block4_conv1(x)))\n",
    "        x = F.relu(self.bn4_2(self.block4_conv2(x)))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        fc2_output = F.relu(self.fc2(x))\n",
    "        x = self.fc3(fc2_output)\n",
    "        x = F.relu(x)\n",
    "        return x, fc2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c007dbed-76f7-4ca4-9e59-897e84e7ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the KD loss function with nn.L1Loss for hard loss\n",
    "def kd_loss_function(student_outputs, student_fc_outputs, teacher_fc_outputs, labels, alpha, T):\n",
    "    \"\"\"\n",
    "    Compute the knowledge-distillation (KD) loss given outputs, labels.\n",
    "    \"Hyperparameters\": temperature and alpha\n",
    "    \"\"\"\n",
    "    hard_loss = nn.L1Loss()(student_outputs, labels)  # Student's loss with the true labels\n",
    "    soft_loss = nn.KLDivLoss()(F.log_softmax(student_fc_outputs / T, dim=1), \n",
    "                               F.softmax(teacher_fc_outputs / T, dim=1)) * (T * T)  # Distillation loss\n",
    "    return alpha * hard_loss + (1. - alpha) * soft_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "853c4114-355d-4003-848d-8dff5ece39f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with knowledge distillation\n",
    "def train_kd(student_model, teacher_model, train_loader, test_loader, num_epochs=10, lr=0.001, alpha=0.5, T=2.0):\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(student_model.parameters(), lr=lr)\n",
    "\n",
    "    best_mae = float('inf')  # Initialize the best MAE to a high value\n",
    "    best_model_path = 'best_student_model.pth'  # Path to save the best model\n",
    "\n",
    "    print(\"Start Training with Knowledge Distillation:\")\n",
    "    for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
    "        running_loss = 0.0  # Initialize the running loss for each epoch\n",
    "        total_mae = 0.0  # Initialize the running MAE for each epoch\n",
    "        total = 0\n",
    "\n",
    "        student_model.train()  # Set the network to training mode\n",
    "        teacher_model.eval()  # Set the teacher network to evaluation mode\n",
    "\n",
    "        for data in train_loader:\n",
    "            # Get the inputs and labels from the data loader\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Move the inputs and labels to the specified device (CPU or GPU)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients to avoid accumulation from previous iterations\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform the forward pass: compute the network's outputs\n",
    "            student_outputs, student_fc2 = student_model(inputs)\n",
    "            with torch.no_grad():\n",
    "                _, teacher_fc2 = teacher_model(inputs)\n",
    "                #print(teacher_fc2.shape)\n",
    "\n",
    "            # Compute the loss using the KD criterion\n",
    "            loss = kd_loss_function(student_outputs, student_fc2, teacher_fc2, labels, alpha, T)\n",
    "\n",
    "            # Perform the backward pass: compute the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the network parameters using the optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for the current mini-batch\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate MAE for the current mini-batch\n",
    "            total_mae += mean_absolute_error(student_outputs.cpu(), labels.cpu()) * inputs.size(0)\n",
    "            total += inputs.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_mae = total_mae / total\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        student_model.eval()  # Set the network to evaluation mode\n",
    "        test_mae = 0.0\n",
    "        test_loss = 0.0\n",
    "        total_test = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs, _ = student_model(inputs)\n",
    "                loss = nn.L1Loss()(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                test_mae += mean_absolute_error(outputs.cpu(), labels.cpu()) * inputs.size(0)\n",
    "                total_test += inputs.size(0)\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_mae /= total_test\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "              f'Training Loss: {train_loss:.4f}, Training MAE: {train_mae:.4f}, '\n",
    "              f'Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}')\n",
    "\n",
    "        # Check if this is the best model so far\n",
    "        if test_mae < best_mae:\n",
    "            best_mae = test_mae\n",
    "            torch.save(student_model.state_dict(), best_model_path)\n",
    "            print(f'Saved model with test MAE: {best_mae:.4f}')\n",
    "\n",
    "    print('Finished Training')\n",
    "    return student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3ef6476d-f925-4c95-89fe-f808d1e14ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the teacher and student models\n",
    "teacher_model = TeacherModel().to(device)\n",
    "student_model = StudentModel().to(device)\n",
    "\n",
    "# Load the pre-trained teacher model\n",
    "teacher_model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ac9d68eb-2863-485e-88ab-8f9a19642212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training with Knowledge Distillation:\n",
      "Epoch [1/100], Training Loss: 9.6547, Training MAE: 18.9748, Test Loss: 14.7491, Test MAE: 14.7491\n",
      "Saved model with test MAE: 14.7491\n",
      "Epoch [2/100], Training Loss: 5.0733, Training MAE: 9.8011, Test Loss: 6.0533, Test MAE: 6.0533\n",
      "Saved model with test MAE: 6.0533\n",
      "Epoch [3/100], Training Loss: 5.0288, Training MAE: 9.7203, Test Loss: 7.0403, Test MAE: 7.0403\n",
      "Epoch [4/100], Training Loss: 4.7529, Training MAE: 9.1759, Test Loss: 7.9691, Test MAE: 7.9691\n",
      "Epoch [5/100], Training Loss: 4.7474, Training MAE: 9.1704, Test Loss: 6.0498, Test MAE: 6.0498\n",
      "Saved model with test MAE: 6.0498\n",
      "Epoch [6/100], Training Loss: 4.7735, Training MAE: 9.2204, Test Loss: 7.8309, Test MAE: 7.8309\n",
      "Epoch [7/100], Training Loss: 4.6572, Training MAE: 8.9949, Test Loss: 6.0430, Test MAE: 6.0430\n",
      "Saved model with test MAE: 6.0430\n",
      "Epoch [8/100], Training Loss: 4.6278, Training MAE: 8.9367, Test Loss: 6.1105, Test MAE: 6.1105\n",
      "Epoch [9/100], Training Loss: 4.6954, Training MAE: 9.0703, Test Loss: 5.8768, Test MAE: 5.8768\n",
      "Saved model with test MAE: 5.8768\n",
      "Epoch [10/100], Training Loss: 4.5590, Training MAE: 8.8000, Test Loss: 5.3797, Test MAE: 5.3797\n",
      "Saved model with test MAE: 5.3797\n",
      "Epoch [11/100], Training Loss: 4.6744, Training MAE: 9.0330, Test Loss: 5.2961, Test MAE: 5.2961\n",
      "Saved model with test MAE: 5.2961\n",
      "Epoch [12/100], Training Loss: 4.5376, Training MAE: 8.7601, Test Loss: 8.0392, Test MAE: 8.0392\n",
      "Epoch [13/100], Training Loss: 4.3529, Training MAE: 8.3872, Test Loss: 7.6001, Test MAE: 7.6001\n",
      "Epoch [14/100], Training Loss: 4.3943, Training MAE: 8.4798, Test Loss: 6.0153, Test MAE: 6.0153\n",
      "Epoch [15/100], Training Loss: 4.3272, Training MAE: 8.3437, Test Loss: 8.2265, Test MAE: 8.2265\n",
      "Epoch [16/100], Training Loss: 4.2589, Training MAE: 8.2061, Test Loss: 6.6943, Test MAE: 6.6943\n",
      "Epoch [17/100], Training Loss: 4.3813, Training MAE: 8.4569, Test Loss: 5.9446, Test MAE: 5.9446\n",
      "Epoch [18/100], Training Loss: 4.2844, Training MAE: 8.2623, Test Loss: 7.6389, Test MAE: 7.6389\n",
      "Epoch [19/100], Training Loss: 4.2282, Training MAE: 8.1512, Test Loss: 5.5986, Test MAE: 5.5986\n",
      "Epoch [20/100], Training Loss: 4.2394, Training MAE: 8.1756, Test Loss: 5.2961, Test MAE: 5.2961\n",
      "Saved model with test MAE: 5.2961\n",
      "Epoch [21/100], Training Loss: 4.1460, Training MAE: 7.9831, Test Loss: 7.7560, Test MAE: 7.7560\n",
      "Epoch [22/100], Training Loss: 4.2033, Training MAE: 8.1059, Test Loss: 5.1399, Test MAE: 5.1399\n",
      "Saved model with test MAE: 5.1399\n",
      "Epoch [23/100], Training Loss: 4.1106, Training MAE: 7.9181, Test Loss: 6.2843, Test MAE: 6.2843\n",
      "Epoch [24/100], Training Loss: 4.0895, Training MAE: 7.8778, Test Loss: 6.0046, Test MAE: 6.0046\n",
      "Epoch [25/100], Training Loss: 4.0317, Training MAE: 7.7579, Test Loss: 6.0706, Test MAE: 6.0706\n",
      "Epoch [26/100], Training Loss: 4.1364, Training MAE: 7.9666, Test Loss: 5.3348, Test MAE: 5.3348\n",
      "Epoch [27/100], Training Loss: 4.0583, Training MAE: 7.8189, Test Loss: 6.2618, Test MAE: 6.2618\n",
      "Epoch [28/100], Training Loss: 4.0087, Training MAE: 7.7207, Test Loss: 5.4948, Test MAE: 5.4948\n",
      "Epoch [29/100], Training Loss: 4.0526, Training MAE: 7.8099, Test Loss: 5.3012, Test MAE: 5.3012\n",
      "Epoch [30/100], Training Loss: 3.9560, Training MAE: 7.6159, Test Loss: 7.3844, Test MAE: 7.3844\n",
      "Epoch [31/100], Training Loss: 3.9161, Training MAE: 7.5333, Test Loss: 6.6746, Test MAE: 6.6746\n",
      "Epoch [32/100], Training Loss: 3.8995, Training MAE: 7.5035, Test Loss: 5.8642, Test MAE: 5.8642\n",
      "Epoch [33/100], Training Loss: 4.0080, Training MAE: 7.7227, Test Loss: 5.6552, Test MAE: 5.6552\n",
      "Epoch [34/100], Training Loss: 3.9123, Training MAE: 7.5264, Test Loss: 5.4064, Test MAE: 5.4064\n",
      "Epoch [35/100], Training Loss: 3.8958, Training MAE: 7.4939, Test Loss: 7.5878, Test MAE: 7.5878\n",
      "Epoch [36/100], Training Loss: 3.8834, Training MAE: 7.4745, Test Loss: 5.3187, Test MAE: 5.3187\n",
      "Epoch [37/100], Training Loss: 3.9772, Training MAE: 7.6624, Test Loss: 6.0377, Test MAE: 6.0377\n",
      "Epoch [38/100], Training Loss: 3.9258, Training MAE: 7.5596, Test Loss: 7.2275, Test MAE: 7.2275\n",
      "Epoch [39/100], Training Loss: 3.9139, Training MAE: 7.5388, Test Loss: 5.9377, Test MAE: 5.9377\n",
      "Epoch [40/100], Training Loss: 3.9403, Training MAE: 7.5897, Test Loss: 5.1643, Test MAE: 5.1643\n",
      "Epoch [41/100], Training Loss: 3.8820, Training MAE: 7.4722, Test Loss: 5.6207, Test MAE: 5.6207\n",
      "Epoch [42/100], Training Loss: 3.9052, Training MAE: 7.5221, Test Loss: 5.3045, Test MAE: 5.3045\n",
      "Epoch [43/100], Training Loss: 3.8566, Training MAE: 7.4234, Test Loss: 5.0800, Test MAE: 5.0800\n",
      "Saved model with test MAE: 5.0800\n",
      "Epoch [44/100], Training Loss: 3.8636, Training MAE: 7.4383, Test Loss: 5.6528, Test MAE: 5.6528\n",
      "Epoch [45/100], Training Loss: 3.9334, Training MAE: 7.5798, Test Loss: 5.5137, Test MAE: 5.5137\n",
      "Epoch [46/100], Training Loss: 3.8009, Training MAE: 7.3176, Test Loss: 5.2528, Test MAE: 5.2528\n",
      "Epoch [47/100], Training Loss: 3.8016, Training MAE: 7.3164, Test Loss: 10.6719, Test MAE: 10.6719\n",
      "Epoch [48/100], Training Loss: 3.8319, Training MAE: 7.3808, Test Loss: 7.2746, Test MAE: 7.2746\n",
      "Epoch [49/100], Training Loss: 3.8986, Training MAE: 7.5131, Test Loss: 9.3762, Test MAE: 9.3762\n",
      "Epoch [50/100], Training Loss: 3.8166, Training MAE: 7.3503, Test Loss: 5.6133, Test MAE: 5.6133\n",
      "Epoch [51/100], Training Loss: 3.7693, Training MAE: 7.2545, Test Loss: 5.5890, Test MAE: 5.5890\n",
      "Epoch [52/100], Training Loss: 3.8248, Training MAE: 7.3644, Test Loss: 9.5075, Test MAE: 9.5075\n",
      "Epoch [53/100], Training Loss: 3.7472, Training MAE: 7.2142, Test Loss: 5.5113, Test MAE: 5.5113\n",
      "Epoch [54/100], Training Loss: 3.7460, Training MAE: 7.2112, Test Loss: 5.4117, Test MAE: 5.4117\n",
      "Epoch [55/100], Training Loss: 3.7862, Training MAE: 7.2917, Test Loss: 6.6746, Test MAE: 6.6746\n",
      "Epoch [56/100], Training Loss: 3.7875, Training MAE: 7.2914, Test Loss: 5.2930, Test MAE: 5.2930\n",
      "Epoch [57/100], Training Loss: 3.7992, Training MAE: 7.3194, Test Loss: 5.6478, Test MAE: 5.6478\n",
      "Epoch [58/100], Training Loss: 3.8286, Training MAE: 7.3762, Test Loss: 5.9378, Test MAE: 5.9378\n",
      "Epoch [59/100], Training Loss: 3.7707, Training MAE: 7.2571, Test Loss: 7.0932, Test MAE: 7.0932\n",
      "Epoch [60/100], Training Loss: 3.8208, Training MAE: 7.3649, Test Loss: 5.0591, Test MAE: 5.0591\n",
      "Saved model with test MAE: 5.0591\n",
      "Epoch [61/100], Training Loss: 3.7393, Training MAE: 7.2024, Test Loss: 5.2118, Test MAE: 5.2118\n",
      "Epoch [62/100], Training Loss: 3.7362, Training MAE: 7.1939, Test Loss: 5.3503, Test MAE: 5.3503\n",
      "Epoch [63/100], Training Loss: 3.7420, Training MAE: 7.2057, Test Loss: 6.9000, Test MAE: 6.9000\n",
      "Epoch [64/100], Training Loss: 3.7885, Training MAE: 7.3022, Test Loss: 6.8513, Test MAE: 6.8513\n",
      "Epoch [65/100], Training Loss: 3.7395, Training MAE: 7.2033, Test Loss: 5.4666, Test MAE: 5.4666\n",
      "Epoch [66/100], Training Loss: 3.7544, Training MAE: 7.2344, Test Loss: 5.1691, Test MAE: 5.1691\n",
      "Epoch [67/100], Training Loss: 3.7095, Training MAE: 7.1446, Test Loss: 5.9233, Test MAE: 5.9233\n",
      "Epoch [68/100], Training Loss: 3.7343, Training MAE: 7.1967, Test Loss: 7.7946, Test MAE: 7.7946\n",
      "Epoch [69/100], Training Loss: 3.7211, Training MAE: 7.1682, Test Loss: 5.6026, Test MAE: 5.6026\n",
      "Epoch [70/100], Training Loss: 3.7703, Training MAE: 7.2692, Test Loss: 5.3396, Test MAE: 5.3396\n",
      "Epoch [71/100], Training Loss: 3.6975, Training MAE: 7.1244, Test Loss: 5.4757, Test MAE: 5.4757\n",
      "Epoch [72/100], Training Loss: 3.6897, Training MAE: 7.1063, Test Loss: 5.2891, Test MAE: 5.2891\n",
      "Epoch [73/100], Training Loss: 3.6931, Training MAE: 7.1151, Test Loss: 6.2543, Test MAE: 6.2543\n",
      "Epoch [74/100], Training Loss: 3.6351, Training MAE: 7.0000, Test Loss: 7.7903, Test MAE: 7.7903\n",
      "Epoch [75/100], Training Loss: 3.6683, Training MAE: 7.0642, Test Loss: 5.2629, Test MAE: 5.2629\n",
      "Epoch [76/100], Training Loss: 3.6988, Training MAE: 7.1301, Test Loss: 5.8567, Test MAE: 5.8567\n",
      "Epoch [77/100], Training Loss: 3.6663, Training MAE: 7.0618, Test Loss: 6.3146, Test MAE: 6.3146\n",
      "Epoch [78/100], Training Loss: 3.6281, Training MAE: 6.9872, Test Loss: 7.1626, Test MAE: 7.1626\n",
      "Epoch [79/100], Training Loss: 3.6383, Training MAE: 7.0054, Test Loss: 5.6891, Test MAE: 5.6891\n",
      "Epoch [80/100], Training Loss: 3.6361, Training MAE: 7.0063, Test Loss: 5.1438, Test MAE: 5.1438\n",
      "Epoch [81/100], Training Loss: 3.6395, Training MAE: 7.0132, Test Loss: 5.0651, Test MAE: 5.0651\n",
      "Epoch [82/100], Training Loss: 3.6467, Training MAE: 7.0272, Test Loss: 5.2853, Test MAE: 5.2853\n",
      "Epoch [83/100], Training Loss: 3.6250, Training MAE: 6.9844, Test Loss: 5.5743, Test MAE: 5.5743\n",
      "Epoch [84/100], Training Loss: 3.6148, Training MAE: 6.9642, Test Loss: 5.2735, Test MAE: 5.2735\n",
      "Epoch [85/100], Training Loss: 3.6260, Training MAE: 6.9877, Test Loss: 5.2320, Test MAE: 5.2320\n",
      "Epoch [86/100], Training Loss: 3.6428, Training MAE: 7.0205, Test Loss: 5.0592, Test MAE: 5.0592\n",
      "Epoch [87/100], Training Loss: 3.6004, Training MAE: 6.9375, Test Loss: 5.4629, Test MAE: 5.4629\n",
      "Epoch [88/100], Training Loss: 3.5895, Training MAE: 6.9149, Test Loss: 5.8646, Test MAE: 5.8646\n",
      "Epoch [91/100], Training Loss: 3.4961, Training MAE: 6.9887, Test Loss: 6.0373, Test MAE: 6.0373\n",
      "Epoch [92/100], Training Loss: 3.4675, Training MAE: 6.9352, Test Loss: 5.1269, Test MAE: 5.1269\n",
      "Epoch [93/100], Training Loss: 3.4166, Training MAE: 6.8331, Test Loss: 5.7462, Test MAE: 5.7462\n",
      "Epoch [94/100], Training Loss: 3.4568, Training MAE: 6.9106, Test Loss: 5.6339, Test MAE: 5.6339\n",
      "Epoch [95/100], Training Loss: 3.4222, Training MAE: 6.8383, Test Loss: 9.1010, Test MAE: 9.1010\n",
      "Epoch [96/100], Training Loss: 3.4503, Training MAE: 6.9014, Test Loss: 5.6837, Test MAE: 5.6837\n",
      "Epoch [97/100], Training Loss: 3.4191, Training MAE: 6.8360, Test Loss: 5.2832, Test MAE: 5.2832\n",
      "Epoch [98/100], Training Loss: 3.4237, Training MAE: 6.8474, Test Loss: 5.1681, Test MAE: 5.1681\n",
      "Epoch [99/100], Training Loss: 3.4319, Training MAE: 6.8634, Test Loss: 5.0996, Test MAE: 5.0996\n",
      "Epoch [100/100], Training Loss: 3.4412, Training MAE: 6.8825, Test Loss: 6.0691, Test MAE: 6.0691\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StudentModel(\n",
       "  (upsample): Upsample(scale_factor=(1.0, 5.0), mode=nearest)\n",
       "  (conv1): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block1_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block1_conv1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block1_conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block2_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block2_conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block2_conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block3_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block3_conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block3_conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block4_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block4_conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn4_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block4_conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn4_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=1152, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train the student model with knowledge distillation\n",
    "train_kd(student_model, teacher_model, train_loader, test_loader, num_epochs=100, lr=0.0001, alpha=0.5, T=2.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
