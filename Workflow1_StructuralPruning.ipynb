{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ccb3c3-372e-4db7-a0f9-518deccf2963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU ID:0,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run /Desktop/Share/CUDA_DEVICE_setup.py -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a296cea-5b34-4879-bfa8-85de6294e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"  # set as the using GPU ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86e28d6f-ef3b-4358-bf0d-b0b916df2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2786247-d496-4eb3-a928-68965114bf0b",
   "metadata": {},
   "source": [
    "# Build the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97daa74c-3c45-4cc5-b219-55ac6054448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCaloDataset(Dataset):\n",
    "    def __init__(self, data_path, target_path, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with data and target paths.\n",
    "        \n",
    "        Parameters:\n",
    "        data_path (str): Path to the .npy file containing image data.\n",
    "        target_path (str): Path to the .npy file containing target data.\n",
    "        transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = np.load(data_path).astype(np.float32)\n",
    "        self.targets = np.load(target_path).astype(np.float32)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        # Permute the sample to have the channel dimension first\n",
    "        sample = np.transpose(sample, (2, 0, 1))  # From [56, 11, 4] to [4, 56, 11]\n",
    "\n",
    "        target = np.expand_dims(target, -1)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e556ba43-7769-4c7f-9aad-95f8fc08976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define file paths\n",
    "base_path = '/Desktop/CodeFolder/PCS/dataset/DeepCalo100K'\n",
    "#base_path = '.'\n",
    "train_data_path = f'{base_path}/dcalo_img_train.npy'\n",
    "train_target_path = f'{base_path}/dcalo_img_train_target.npy'\n",
    "val_data_path = f'{base_path}/dcalo_img_val.npy'\n",
    "val_target_path = f'{base_path}/dcalo_img_val_target.npy'\n",
    "test_data_path = f'{base_path}/dcalo_img_test.npy'\n",
    "test_target_path = f'{base_path}/dcalo_img_test_target.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22f0a877-86bd-4b7f-93c1-fff6134881b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = DeepCaloDataset(train_data_path, train_target_path)\n",
    "val_dataset = DeepCaloDataset(val_data_path, val_target_path)\n",
    "test_dataset = DeepCaloDataset(test_data_path, test_target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa4eed4f-d24e-40f0-9eff-b757fca61619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "tr_batch_size = 64\n",
    "test_batch_size = 16\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=tr_batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=test_batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d74ab7-5b3f-49b3-8e93-e88bc3c05e2f",
   "metadata": {},
   "source": [
    "## Have a look at images for DeepCalo\n",
    "Each image has a shape of (56, 11, 4), 4 channel currosponding to 4 layers if electromegnatic calorememter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f06f538-9601-46f9-ad70-86cfc1fba775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6550099f-3981-4bed-b314-4a993ef3e30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 56, 11])\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the first batch \n",
    "data_iter = iter(test_loader)\n",
    "batch = next(data_iter)\n",
    "\n",
    "# Extract samples and targets\n",
    "# Show the first 2 data\n",
    "samples, targets = batch\n",
    "samples, targets = samples[:2], targets[:2]\n",
    "\n",
    "print(samples.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26dc9b65-6b8a-4375-8f2e-d234bc7ba9bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First image: \n",
      "Target: 146.55\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seocnd image: \n",
      "Target: 101.04\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"First image: \")\n",
    "print(f\"Target: {targets[0].item():.2f}\")\n",
    "\n",
    "# Plot each channel separately\n",
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.imshow(samples[0][i])\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Channel {i + 1}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Seocnd image: \")\n",
    "print(f\"Target: {targets[1].item():.2f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.imshow(samples[1][i])\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Channel {i + 1}\")\n",
    "plt.show()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714962ed-5e60-49a5-8751-b9fe7fc4ae15",
   "metadata": {},
   "source": [
    "# DeepCalo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "963c4400-1285-4bd3-84d7-50038edacfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eae85c70-1fee-4d37-9aec-6c930622f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eac5afad-c110-4e78-9187-40082a68ed1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c88a0e0-0312-455e-8ff5-3bd9089337cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import brevitas.nn as qnn\n",
    "\n",
    "class DeepCaloModelFull(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCaloModelFull, self).__init__()\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=(1, 5), mode='nearest')\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(4, 16, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.block1_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block1_conv1 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(32)\n",
    "        self.block1_conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.block2_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block2_conv1 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(64)\n",
    "        self.block2_conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.block3_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block3_conv1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(128)\n",
    "        self.block3_conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.block4_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block4_conv1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4_1 = nn.BatchNorm2d(256)\n",
    "        self.block4_conv2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn4_2 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        x = self.block1_pool(x)\n",
    "        x = F.relu(self.bn1_1(self.block1_conv1(x)))\n",
    "        x = F.relu(self.bn1_2(self.block1_conv2(x)))\n",
    "\n",
    "        x = self.block2_pool(x)\n",
    "        x = F.relu(self.bn2_1(self.block2_conv1(x)))\n",
    "        x = F.relu(self.bn2_2(self.block2_conv2(x)))\n",
    "\n",
    "        x = self.block3_pool(x)\n",
    "        x = F.relu(self.bn3_1(self.block3_conv1(x)))\n",
    "        x = F.relu(self.bn3_2(self.block3_conv2(x)))\n",
    "\n",
    "        x = self.block4_pool(x)\n",
    "        x = F.relu(self.bn4_1(self.block4_conv1(x)))\n",
    "        x = F.relu(self.bn4_2(self.block4_conv2(x)))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42ff295d-c1c4-45fd-949e-53559304e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_calo_model = DeepCaloModelFull().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "512b3a11-8fc0-41fd-87c8-3dd5530a99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0aa9c8dc-5c0f-4602-904d-ad43f6069b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "DeepCaloModelFull                        --\n",
       "├─Upsample: 1-1                          --\n",
       "├─Conv2d: 1-2                            1,616\n",
       "├─BatchNorm2d: 1-3                       32\n",
       "├─MaxPool2d: 1-4                         --\n",
       "├─Conv2d: 1-5                            4,640\n",
       "├─BatchNorm2d: 1-6                       64\n",
       "├─Conv2d: 1-7                            9,248\n",
       "├─BatchNorm2d: 1-8                       64\n",
       "├─MaxPool2d: 1-9                         --\n",
       "├─Conv2d: 1-10                           18,496\n",
       "├─BatchNorm2d: 1-11                      128\n",
       "├─Conv2d: 1-12                           36,928\n",
       "├─BatchNorm2d: 1-13                      128\n",
       "├─MaxPool2d: 1-14                        --\n",
       "├─Conv2d: 1-15                           73,856\n",
       "├─BatchNorm2d: 1-16                      256\n",
       "├─Conv2d: 1-17                           147,584\n",
       "├─BatchNorm2d: 1-18                      256\n",
       "├─MaxPool2d: 1-19                        --\n",
       "├─Conv2d: 1-20                           295,168\n",
       "├─BatchNorm2d: 1-21                      512\n",
       "├─Conv2d: 1-22                           590,080\n",
       "├─BatchNorm2d: 1-23                      512\n",
       "├─Linear: 1-24                           590,080\n",
       "├─Linear: 1-25                           65,792\n",
       "├─Linear: 1-26                           257\n",
       "=================================================================\n",
       "Total params: 1,835,697\n",
       "Trainable params: 1,835,697\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(deep_calo_model, input_shape = [1, 56, 11, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77ebe6fa-71f8-48e1-9bba-b8e8ab39f0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepCaloModelFull(\n",
       "  (upsample): Upsample(scale_factor=(1.0, 5.0), mode=nearest)\n",
       "  (conv1): Conv2d(4, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block1_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block1_conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block1_conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block2_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block2_conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block2_conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block3_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block3_conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block3_conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block4_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block4_conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn4_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block4_conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn4_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=2304, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_calo_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5ba58-e014-4074-9157-71566b6112f5",
   "metadata": {},
   "source": [
    "# Train the initial model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7fe38e9-0e36-41eb-81c4-fb1463e36d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return torch.mean(torch.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad4c8f2a-7a40-4899-838e-92d3166612f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = 'best_deepalo_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6f590a3-0f39-4c6c-bdd3-099d44ca4054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(deep_calo_model, train_loader, test_loader, num_epochs=10, lr=0.001, best_model_path = 'best_deepalo_model.pth'):\n",
    "    # Define the loss function and the optimizer\n",
    "    criterion = nn.L1Loss()  # Mean Absolute Error (MAE) Loss\n",
    "    optimizer = optim.Adam(deep_calo_model.parameters(), lr=lr)\n",
    "\n",
    "    train_accuracies = []\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    test_losses = []\n",
    "\n",
    "    best_loss = float('inf')  # Initialize the best loss to a high value\n",
    "      # Path to save the best model\n",
    "\n",
    "    print(\"Start Training:\")\n",
    "    for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
    "        running_loss = 0.0  # Initialize the running loss for each epoch\n",
    "        total_mae = 0.0  # Initialize the running MAE for each epoch\n",
    "        total = 0\n",
    "\n",
    "        deep_calo_model.train()  # Set the network to training mode\n",
    "\n",
    "        for data in train_loader:\n",
    "            # Get the inputs and labels from the data loader\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Move the inputs and labels to the specified device (CPU or GPU)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients to avoid accumulation from previous iterations\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform the forward pass: compute the network's outputs\n",
    "            outputs = deep_calo_model(inputs)\n",
    "\n",
    "            # Compute the loss using the criterion\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Perform the backward pass: compute the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the network parameters using the optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for the current mini-batch\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate MAE for the current mini-batch\n",
    "            total_mae += mean_absolute_error(outputs.cpu(), labels.cpu()) * inputs.size(0)\n",
    "            total += inputs.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_mae = total_mae / total\n",
    "        train_accuracies.append(train_mae)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        deep_calo_model.eval()  # Set the network to evaluation mode\n",
    "        test_mae = 0.0\n",
    "        test_loss = 0.0\n",
    "        total_test = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = deep_calo_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                test_mae += mean_absolute_error(outputs.cpu(), labels.cpu()) * inputs.size(0)\n",
    "                total_test += inputs.size(0)\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_mae /= total_test\n",
    "        test_accuracies.append(test_mae)\n",
    "        test_losses.append(test_loss)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "              f'Training Loss: {train_loss:.4f}, Training MAE: {train_mae:.4f}, '\n",
    "              f'Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}')\n",
    "\n",
    "        # Check if this is the best model so far\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            torch.save(deep_calo_model.state_dict(), best_model_path)\n",
    "            print(f'Saved model with test loss: {best_loss:.4f}')\n",
    "\n",
    "    print('Finished Training')\n",
    "    return train_accuracies, train_losses, test_accuracies, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "974df5f5-fe3d-478e-9845-ecff19cde8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model performance on the test data for first 5 batches\n",
    "def test_model(model, test_loader, device, num_batches=5):\n",
    "\n",
    "    criterion = nn.L1Loss()  # Mean Absolute Error (MAE) Loss\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_mae = 0.0\n",
    "    test_loss = 0.0\n",
    "    total_test = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            if batch_count >= num_batches:\n",
    "                break\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            test_mae += mean_absolute_error(outputs.cpu(), labels.cpu()) * inputs.size(0)\n",
    "            total_test += inputs.size(0)\n",
    "            batch_count += 1\n",
    "\n",
    "            print(f'Batch {batch_count}, Test MAE: {mean_absolute_error(outputs.cpu(), labels.cpu()):.4f}, Test Loss: {loss.item():.4f}')\n",
    "\n",
    "    test_loss /= batch_count\n",
    "    test_mae /= total_test\n",
    "    print(f'Average Test MAE for first {num_batches} batches: {test_mae:.4f}')\n",
    "    print(f'Average Test Loss for first {num_batches} batches: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ca80832-a196-4b40-911a-eb602e243115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained DeepCalo:  best_deepalo_model.pth\n",
      "Batch 1, Test MAE: 7.8209, Test Loss: 7.8209\n",
      "Batch 2, Test MAE: 4.6491, Test Loss: 4.6491\n",
      "Batch 3, Test MAE: 8.6994, Test Loss: 8.6994\n",
      "Batch 4, Test MAE: 4.3375, Test Loss: 4.3375\n",
      "Batch 5, Test MAE: 3.7579, Test Loss: 3.7579\n",
      "Average Test MAE for first 5 batches: 5.8530\n",
      "Average Test Loss for first 5 batches: 5.8530\n"
     ]
    }
   ],
   "source": [
    "# train 第一遍\n",
    "\n",
    "load_pretrain = True\n",
    "if load_pretrain:\n",
    "    print(\"Load pretrained DeepCalo: \", best_model_path)\n",
    "    deep_calo_model = DeepCaloModelFull().to(device)\n",
    "    deep_calo_model.load_state_dict(torch.load(best_model_path))\n",
    "    test_model(deep_calo_model, test_loader, device, num_batches=5)\n",
    "else:\n",
    "    train_accuracies, train_losses, test_accuracies, test_losses = \\\n",
    "    train_model(deep_calo_model, train_loader, test_loader, num_epochs=100, lr = 0.001, best_model_path = best_model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06685d5b-0793-4682-b512-17184bd56153",
   "metadata": {},
   "source": [
    "# Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4241c55-58b8-479f-b252-1462a61826a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepCaloModelFull(\n",
      "  (upsample): Upsample(scale_factor=(1.0, 5.0), mode=nearest)\n",
      "  (conv1): Conv2d(4, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block1_conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block2_conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block3_conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block4_conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=2304, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = deep_calo_model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b473f308-ec94-4fba-a4bd-616eb71f3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_pruning as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac9d68eb-2863-485e-88ab-8f9a19642212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimportance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMagnitudeImportance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgroup_reduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnormalizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget_types\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'torch.nn.modules.conv._ConvNd'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'torch.nn.modules.linear.Linear'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'torch.nn.modules.batchnorm._BatchNorm'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'torch.nn.modules.normalization.LayerNorm'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "A general implementation of magnitude importance. By default, it calculates the group L2-norm for each channel/dim.\n",
       "It supports several variants like:\n",
       "    - Standard L1-norm of the first layer in a group: MagnitudeImportance(p=1, normalizer=None, group_reduction=\"first\")\n",
       "    - Group L1-Norm: MagnitudeImportance(p=1, normalizer=None, group_reduction=\"mean\")\n",
       "    - BN Scaling Factor: MagnitudeImportance(p=1, normalizer=None, group_reduction=\"mean\", target_types=[nn.modules.batchnorm._BatchNorm])\n",
       "\n",
       "Args:\n",
       "    * p (int): the norm degree. Default: 2\n",
       "    * group_reduction (str): the reduction method for group importance. Default: \"mean\"\n",
       "    * normalizer (str): the normalization method for group importance. Default: \"mean\"\n",
       "    * target_types (list): the target types for importance calculation. Default: [nn.modules.conv._ConvNd, nn.Linear, nn.modules.batchnorm._BatchNorm]\n",
       "\n",
       "Example:\n",
       "\n",
       "    It accepts a group as inputs, and return a 1-D tensor with the same length as the number of channels.\n",
       "    All groups must be pruned simultaneously and thus their importance should be accumulated across channel groups.\n",
       "    \n",
       "    ```python\n",
       "        DG = tp.DependencyGraph().build_dependency(model, example_inputs=torch.randn(1,3,224,224)) \n",
       "        group = DG.get_pruning_group( model.conv1, tp.prune_conv_out_channels, idxs=[2, 6, 9] )    \n",
       "        scorer = GroupNormImportance()    \n",
       "        imp_score = scorer(group)    \n",
       "        #imp_score is a 1-D tensor with length 3 for channels [2, 6, 9]  \n",
       "        min_score = imp_score.min() \n",
       "    ``` \n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/USEGPU/lib/python3.8/site-packages/torch_pruning/pruner/importance.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?tp.importance.MagnitudeImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbb67f4b-b764-4b39-81be-3646f30c2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. importance criterion for parameter selections\n",
    "imp = tp.importance.MagnitudeImportance(p=2, group_reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3321451-1770-469e-880f-c7beb0b581f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_layers = []\n",
    "for m in model.modules():\n",
    "    # ignore the final layer only\n",
    "    if isinstance(m, torch.nn.Linear) and m.out_features == 1:\n",
    "    #if isinstance(m, torch.nn.Linear):\n",
    "        ignored_layers.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "efa6c091-c08b-4e4b-a4ca-64f1fa6cdffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=256, out_features=1, bias=True)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignored_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7fb32e43-35b5-4811-b93e-92626b14a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_inputs = torch.randn(1, 4, 56, 11).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "749be510-f050-4626-bc7e-1ade94415e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepCaloModelFull(\n",
      "  (upsample): Upsample(scale_factor=(1.0, 5.0), mode=nearest)\n",
      "  (conv1): Conv2d(4, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block1_conv1): Conv2d(12, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_conv2): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block2_conv1): Conv2d(25, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_conv2): Conv2d(51, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_2): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block3_conv1): Conv2d(51, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_1): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_conv2): Conv2d(102, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_2): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block4_conv1): Conv2d(102, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_conv2): Conv2d(204, 204, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_2): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=1836, out_features=204, bias=True)\n",
      "  (fc2): Linear(in_features=204, out_features=204, bias=True)\n",
      "  (fc3): Linear(in_features=204, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([1, 1])\n",
      "  Iter 1/1, Params: 1835697.000000 => 1165780.000000 M\n",
      "  Iter 1/1, MACs: 0.04 G => 0.03 G\n",
      "Start Training:\n",
      "Epoch [1/10], Training Loss: 9.1365, Training MAE: 9.1368, Test Loss: 5.9940, Test MAE: 5.9940\n",
      "Saved model with test loss: 5.9940\n",
      "Epoch [2/10], Training Loss: 7.1618, Training MAE: 7.1608, Test Loss: 5.6325, Test MAE: 5.6325\n",
      "Saved model with test loss: 5.6325\n",
      "Epoch [3/10], Training Loss: 7.0053, Training MAE: 6.9973, Test Loss: 6.5283, Test MAE: 6.5283\n",
      "Epoch [4/10], Training Loss: 6.7612, Training MAE: 6.7568, Test Loss: 5.2839, Test MAE: 5.2839\n",
      "Saved model with test loss: 5.2839\n",
      "Epoch [5/10], Training Loss: 6.8012, Training MAE: 6.7976, Test Loss: 5.3079, Test MAE: 5.3079\n",
      "Epoch [6/10], Training Loss: 6.6978, Training MAE: 6.6981, Test Loss: 5.2841, Test MAE: 5.2841\n",
      "Epoch [7/10], Training Loss: 6.7506, Training MAE: 6.7469, Test Loss: 6.5631, Test MAE: 6.5631\n",
      "Epoch [8/10], Training Loss: 6.7319, Training MAE: 6.7309, Test Loss: 6.8269, Test MAE: 6.8269\n",
      "Epoch [9/10], Training Loss: 6.6933, Training MAE: 6.6910, Test Loss: 5.2324, Test MAE: 5.2324\n",
      "Saved model with test loss: 5.2324\n",
      "Epoch [10/10], Training Loss: 6.7253, Training MAE: 6.7244, Test Loss: 5.1024, Test MAE: 5.1024\n",
      "Saved model with test loss: 5.1024\n",
      "Finished Training\n",
      "DeepCaloModelFull(\n",
      "  (upsample): Upsample(scale_factor=(1.0, 5.0), mode=nearest)\n",
      "  (conv1): Conv2d(4, 9, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block1_conv1): Conv2d(9, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block2_conv1): Conv2d(20, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block3_conv1): Conv2d(40, 81, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_1): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_conv2): Conv2d(81, 81, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_2): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block4_conv1): Conv2d(81, 163, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_1): BatchNorm2d(163, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_conv2): Conv2d(163, 163, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_2): BatchNorm2d(163, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=1467, out_features=163, bias=True)\n",
      "  (fc2): Linear(in_features=163, out_features=163, bias=True)\n",
      "  (fc3): Linear(in_features=163, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([1, 1])\n",
      "  Iter 2/1, Params: 1835697.000000 => 741908.000000 M\n",
      "  Iter 2/1, MACs: 0.04 G => 0.02 G\n",
      "Start Training:\n",
      "Epoch [1/10], Training Loss: 10.1632, Training MAE: 10.1594, Test Loss: 6.0870, Test MAE: 6.0870\n",
      "Saved model with test loss: 6.0870\n",
      "Epoch [2/10], Training Loss: 7.2579, Training MAE: 7.2511, Test Loss: 6.9345, Test MAE: 6.9345\n",
      "Epoch [3/10], Training Loss: 7.1455, Training MAE: 7.1437, Test Loss: 6.1177, Test MAE: 6.1177\n",
      "Epoch [4/10], Training Loss: 6.9960, Training MAE: 6.9947, Test Loss: 6.6234, Test MAE: 6.6234\n",
      "Epoch [5/10], Training Loss: 6.9647, Training MAE: 6.9638, Test Loss: 5.4515, Test MAE: 5.4515\n",
      "Saved model with test loss: 5.4515\n",
      "Epoch [6/10], Training Loss: 7.0990, Training MAE: 7.0974, Test Loss: 5.3809, Test MAE: 5.3809\n",
      "Saved model with test loss: 5.3809\n",
      "Epoch [7/10], Training Loss: 6.8293, Training MAE: 6.8268, Test Loss: 7.5878, Test MAE: 7.5878\n",
      "Epoch [1/10], Training Loss: 9.7406, Training MAE: 9.7396, Test Loss: 7.0895, Test MAE: 7.0895\n",
      "Saved model with test loss: 7.0895\n",
      "Epoch [2/10], Training Loss: 7.8439, Training MAE: 7.8430, Test Loss: 8.2274, Test MAE: 8.2274\n",
      "Epoch [3/10], Training Loss: 7.5848, Training MAE: 7.5822, Test Loss: 6.8621, Test MAE: 6.8621\n",
      "Saved model with test loss: 6.8621\n",
      "Epoch [4/10], Training Loss: 7.4675, Training MAE: 7.4686, Test Loss: 6.0467, Test MAE: 6.0467\n",
      "Saved model with test loss: 6.0467\n",
      "Epoch [5/10], Training Loss: 7.2819, Training MAE: 7.2797, Test Loss: 6.1416, Test MAE: 6.1416\n",
      "Epoch [6/10], Training Loss: 7.3247, Training MAE: 7.3255, Test Loss: 5.7889, Test MAE: 5.7889\n",
      "Saved model with test loss: 5.7889\n",
      "Epoch [7/10], Training Loss: 7.2872, Training MAE: 7.2848, Test Loss: 7.2867, Test MAE: 7.2867\n",
      "Epoch [8/10], Training Loss: 7.2517, Training MAE: 7.2526, Test Loss: 5.8289, Test MAE: 5.8289\n",
      "Epoch [9/10], Training Loss: 7.1131, Training MAE: 7.1140, Test Loss: 6.0823, Test MAE: 6.0823\n",
      "Epoch [10/10], Training Loss: 7.0459, Training MAE: 7.0441, Test Loss: 5.6093, Test MAE: 5.6093\n",
      "Saved model with test loss: 5.6093\n",
      "Finished Training\n",
      "DeepCaloModelFull(\n",
      "  (upsample): Upsample(scale_factor=(1.0, 5.0), mode=nearest)\n",
      "  (conv1): Conv2d(4, 5, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block1_conv1): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block2_conv1): Conv2d(12, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_conv2): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block3_conv1): Conv2d(25, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_conv2): Conv2d(51, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_2): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block4_conv1): Conv2d(51, 104, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_1): BatchNorm2d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_conv2): Conv2d(104, 104, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_2): BatchNorm2d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=936, out_features=104, bias=True)\n",
      "  (fc2): Linear(in_features=104, out_features=104, bias=True)\n",
      "  (fc3): Linear(in_features=104, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([1, 1])\n",
      "  Iter 4/1, Params: 1835697.000000 => 300265.000000 M\n",
      "  Iter 4/1, MACs: 0.04 G => 0.01 G\n",
      "Start Training:\n",
      "Epoch [1/10], Training Loss: 10.1704, Training MAE: 10.1714, Test Loss: 6.7263, Test MAE: 6.7263\n",
      "Saved model with test loss: 6.7263\n",
      "Epoch [2/10], Training Loss: 8.1195, Training MAE: 8.1207, Test Loss: 6.5930, Test MAE: 6.5930\n",
      "Saved model with test loss: 6.5930\n",
      "Epoch [3/10], Training Loss: 7.7260, Training MAE: 7.7244, Test Loss: 6.5233, Test MAE: 6.5233\n",
      "Saved model with test loss: 6.5233\n",
      "Epoch [4/10], Training Loss: 7.7012, Training MAE: 7.7011, Test Loss: 6.3127, Test MAE: 6.3127\n",
      "Saved model with test loss: 6.3127\n",
      "Epoch [5/10], Training Loss: 7.5312, Training MAE: 7.5314, Test Loss: 5.8655, Test MAE: 5.8655\n",
      "Saved model with test loss: 5.8655\n",
      "Epoch [6/10], Training Loss: 7.5962, Training MAE: 7.5936, Test Loss: 5.9802, Test MAE: 5.9802\n",
      "Epoch [7/10], Training Loss: 7.5302, Training MAE: 7.5296, Test Loss: 6.4200, Test MAE: 6.4200\n",
      "Epoch [8/10], Training Loss: 7.5288, Training MAE: 7.5275, Test Loss: 6.1887, Test MAE: 6.1887\n",
      "Epoch [9/10], Training Loss: 7.2785, Training MAE: 7.2786, Test Loss: 5.7947, Test MAE: 5.7947\n",
      "Saved model with test loss: 5.7947\n",
      "Epoch [10/10], Training Loss: 7.3669, Training MAE: 7.3639, Test Loss: 6.4899, Test MAE: 6.4899\n",
      "Finished Training\n",
      "DeepCaloModelFull(\n",
      "  (upsample): Upsample(scale_factor=(1.0, 5.0), mode=nearest)\n",
      "  (conv1): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block1_conv1): Conv2d(4, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block2_conv1): Conv2d(9, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block3_conv1): Conv2d(20, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block4_conv1): Conv2d(40, 83, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_1): BatchNorm2d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_conv2): Conv2d(83, 83, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_2): BatchNorm2d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=747, out_features=83, bias=True)\n",
      "  (fc2): Linear(in_features=83, out_features=83, bias=True)\n",
      "  (fc3): Linear(in_features=83, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([1, 1])\n",
      "  Iter 5/1, Params: 1835697.000000 => 190218.000000 M\n",
      "  Iter 5/1, MACs: 0.04 G => 0.00 G\n",
      "Start Training:\n",
      "Epoch [1/10], Training Loss: 13.2086, Training MAE: 13.2098, Test Loss: 7.4636, Test MAE: 7.4636\n",
      "Saved model with test loss: 7.4636\n",
      "Epoch [2/10], Training Loss: 9.1021, Training MAE: 9.0999, Test Loss: 6.8972, Test MAE: 6.8972\n",
      "Saved model with test loss: 6.8972\n",
      "Epoch [3/10], Training Loss: 8.6014, Training MAE: 8.6018, Test Loss: 6.4010, Test MAE: 6.4010\n",
      "Saved model with test loss: 6.4010\n",
      "Epoch [4/10], Training Loss: 8.3953, Training MAE: 8.3956, Test Loss: 7.1939, Test MAE: 7.1939\n",
      "Epoch [5/10], Training Loss: 8.3714, Training MAE: 8.3708, Test Loss: 6.2780, Test MAE: 6.2780\n",
      "Saved model with test loss: 6.2780\n",
      "Epoch [6/10], Training Loss: 8.3014, Training MAE: 8.3021, Test Loss: 5.9757, Test MAE: 5.9757\n",
      "Saved model with test loss: 5.9757\n",
      "Epoch [7/10], Training Loss: 8.1203, Training MAE: 8.1187, Test Loss: 6.3325, Test MAE: 6.3325\n",
      "Epoch [8/10], Training Loss: 8.0943, Training MAE: 8.0943, Test Loss: 5.8466, Test MAE: 5.8466\n",
      "Saved model with test loss: 5.8466\n",
      "Epoch [9/10], Training Loss: 8.0301, Training MAE: 8.0306, Test Loss: 5.9440, Test MAE: 5.9440\n",
      "Epoch [10/10], Training Loss: 7.9464, Training MAE: 7.9466, Test Loss: 5.8996, Test MAE: 5.8996\n",
      "Finished Training\n",
      "DeepCaloModelFull(\n",
      "  (upsample): Upsample(scale_factor=(1.0, 5.0), mode=nearest)\n",
      "  (conv1): Conv2d(4, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block1_conv1): Conv2d(3, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block2_conv1): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block3_conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block4_conv1): Conv2d(32, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_1): BatchNorm2d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_conv2): Conv2d(66, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_2): BatchNorm2d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=594, out_features=66, bias=True)\n",
      "  (fc2): Linear(in_features=66, out_features=66, bias=True)\n",
      "  (fc3): Linear(in_features=66, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([1, 1])\n",
      "  Iter 6/1, Params: 1835697.000000 => 120772.000000 M\n",
      "  Iter 6/1, MACs: 0.04 G => 0.00 G\n",
      "Start Training:\n",
      "Epoch [1/10], Training Loss: 11.8663, Training MAE: 11.8631, Test Loss: 7.1552, Test MAE: 7.1552\n",
      "Saved model with test loss: 7.1552\n",
      "Epoch [2/10], Training Loss: 9.5454, Training MAE: 9.5349, Test Loss: 7.0048, Test MAE: 7.0048\n",
      "Saved model with test loss: 7.0048\n",
      "Epoch [3/10], Training Loss: 9.3297, Training MAE: 9.3180, Test Loss: 6.5862, Test MAE: 6.5862\n",
      "Saved model with test loss: 6.5862\n",
      "Epoch [4/10], Training Loss: 9.0664, Training MAE: 9.0632, Test Loss: 6.2383, Test MAE: 6.2383\n",
      "Saved model with test loss: 6.2383\n",
      "Epoch [5/10], Training Loss: 8.9738, Training MAE: 8.9637, Test Loss: 8.3062, Test MAE: 8.3062\n",
      "Epoch [6/10], Training Loss: 8.8418, Training MAE: 8.8365, Test Loss: 6.2434, Test MAE: 6.2434\n",
      "Epoch [7/10], Training Loss: 8.7821, Training MAE: 8.7833, Test Loss: 5.9959, Test MAE: 5.9959\n",
      "Saved model with test loss: 5.9959\n",
      "Epoch [8/10], Training Loss: 8.8436, Training MAE: 8.8433, Test Loss: 5.9801, Test MAE: 5.9801\n",
      "Saved model with test loss: 5.9801\n",
      "Epoch [9/10], Training Loss: 8.8064, Training MAE: 8.8060, Test Loss: 6.4036, Test MAE: 6.4036\n",
      "Epoch [10/10], Training Loss: 8.6609, Training MAE: 8.6601, Test Loss: 6.4150, Test MAE: 6.4150\n",
      "Finished Training\n",
      "DeepCaloModelFull(\n",
      "  (upsample): Upsample(scale_factor=(1.0, 5.0), mode=nearest)\n",
      "  (conv1): Conv2d(4, 2, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block1_conv1): Conv2d(2, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_conv2): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block2_conv1): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block3_conv1): Conv2d(12, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_conv2): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block4_conv1): Conv2d(25, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_conv2): Conv2d(52, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_2): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=468, out_features=52, bias=True)\n",
      "  (fc2): Linear(in_features=52, out_features=52, bias=True)\n",
      "  (fc3): Linear(in_features=52, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([1, 1])\n",
      "  Iter 7/1, Params: 1835697.000000 => 74479.000000 M\n",
      "  Iter 7/1, MACs: 0.04 G => 0.00 G\n",
      "Start Training:\n",
      "Epoch [1/10], Training Loss: 12.0187, Training MAE: 12.0204, Test Loss: 8.9179, Test MAE: 8.9179\n",
      "Saved model with test loss: 8.9179\n",
      "Epoch [2/10], Training Loss: 9.4096, Training MAE: 9.4070, Test Loss: 6.9593, Test MAE: 6.9593\n",
      "Saved model with test loss: 6.9593\n",
      "Epoch [3/10], Training Loss: 9.1656, Training MAE: 9.1661, Test Loss: 6.2595, Test MAE: 6.2595\n",
      "Saved model with test loss: 6.2595\n",
      "Epoch [4/10], Training Loss: 8.9522, Training MAE: 8.9523, Test Loss: 6.3527, Test MAE: 6.3527\n",
      "Epoch [5/10], Training Loss: 9.0733, Training MAE: 9.0720, Test Loss: 6.2966, Test MAE: 6.2966\n",
      "Epoch [6/10], Training Loss: 8.9852, Training MAE: 8.9835, Test Loss: 6.5825, Test MAE: 6.5825\n",
      "Epoch [7/10], Training Loss: 8.7598, Training MAE: 8.7515, Test Loss: 6.9973, Test MAE: 6.9973\n",
      "Epoch [8/10], Training Loss: 8.6268, Training MAE: 8.6273, Test Loss: 6.0225, Test MAE: 6.0225\n",
      "Saved model with test loss: 6.0225\n",
      "Epoch [9/10], Training Loss: 8.7882, Training MAE: 8.7874, Test Loss: 7.3559, Test MAE: 7.3559\n",
      "Epoch [10/10], Training Loss: 8.7762, Training MAE: 8.7755, Test Loss: 5.9105, Test MAE: 5.9105\n",
      "Saved model with test loss: 5.9105\n",
      "Finished Training\n",
      "DeepCaloModelFull(\n",
      "  (upsample): Upsample(scale_factor=(1.0, 5.0), mode=nearest)\n",
      "  (conv1): Conv2d(4, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block1_conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block2_conv1): Conv2d(4, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block3_conv1): Conv2d(9, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block4_conv1): Conv2d(20, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_1): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_conv2): Conv2d(41, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_2): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=369, out_features=41, bias=True)\n",
      "  (fc2): Linear(in_features=41, out_features=41, bias=True)\n",
      "  (fc3): Linear(in_features=41, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([1, 1])\n",
      "  Iter 8/1, Params: 1835697.000000 => 46443.000000 M\n",
      "  Iter 8/1, MACs: 0.04 G => 0.00 G\n",
      "Start Training:\n",
      "Epoch [1/10], Training Loss: 20.2342, Training MAE: 20.2315, Test Loss: 14.2815, Test MAE: 14.2815\n",
      "Saved model with test loss: 14.2815\n",
      "Epoch [2/10], Training Loss: 16.6751, Training MAE: 16.6760, Test Loss: 13.0309, Test MAE: 13.0309\n",
      "Saved model with test loss: 13.0309\n",
      "Epoch [3/10], Training Loss: 15.4647, Training MAE: 15.4655, Test Loss: 12.5423, Test MAE: 12.5423\n",
      "Saved model with test loss: 12.5423\n",
      "Epoch [4/10], Training Loss: 15.2046, Training MAE: 15.2063, Test Loss: 12.4620, Test MAE: 12.4620\n",
      "Saved model with test loss: 12.4620\n",
      "Epoch [5/10], Training Loss: 14.7571, Training MAE: 14.7522, Test Loss: 12.0361, Test MAE: 12.0361\n",
      "Saved model with test loss: 12.0361\n",
      "Epoch [6/10], Training Loss: 14.4127, Training MAE: 14.4124, Test Loss: 11.8009, Test MAE: 11.8010\n",
      "Saved model with test loss: 11.8009\n",
      "Epoch [7/10], Training Loss: 14.4325, Training MAE: 14.4328, Test Loss: 11.6877, Test MAE: 11.6877\n",
      "Saved model with test loss: 11.6877\n",
      "Epoch [8/10], Training Loss: 14.1895, Training MAE: 14.1868, Test Loss: 11.8002, Test MAE: 11.8002\n",
      "Epoch [9/10], Training Loss: 14.0263, Training MAE: 14.0267, Test Loss: 11.3208, Test MAE: 11.3208\n",
      "Saved model with test loss: 11.3208\n",
      "Epoch [10/10], Training Loss: 13.9045, Training MAE: 13.9050, Test Loss: 11.5190, Test MAE: 11.5190\n",
      "Finished Training\n",
      "DeepCaloModelFull(\n",
      "  (upsample): Upsample(scale_factor=(1.0, 5.0), mode=nearest)\n",
      "  (conv1): Conv2d(4, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block1_conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block2_conv1): Conv2d(3, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block3_conv1): Conv2d(7, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block4_conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=288, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([1, 1])\n",
      "  Iter 9/1, Params: 1835697.000000 => 28662.000000 M\n",
      "  Iter 9/1, MACs: 0.04 G => 0.00 G\n",
      "Start Training:\n",
      "Epoch [1/10], Training Loss: 19.9726, Training MAE: 19.9704, Test Loss: 13.0581, Test MAE: 13.0581\n",
      "Saved model with test loss: 13.0581\n",
      "Epoch [2/10], Training Loss: 15.0640, Training MAE: 15.0607, Test Loss: 12.2446, Test MAE: 12.2446\n",
      "Saved model with test loss: 12.2446\n",
      "Epoch [3/10], Training Loss: 14.7129, Training MAE: 14.7110, Test Loss: 11.9309, Test MAE: 11.9309\n",
      "Saved model with test loss: 11.9309\n",
      "Epoch [4/10], Training Loss: 14.2037, Training MAE: 14.2025, Test Loss: 12.1057, Test MAE: 12.1057\n",
      "Epoch [5/10], Training Loss: 14.0581, Training MAE: 14.0589, Test Loss: 11.1365, Test MAE: 11.1365\n",
      "Saved model with test loss: 11.1365\n",
      "Epoch [6/10], Training Loss: 13.8359, Training MAE: 13.8352, Test Loss: 11.6817, Test MAE: 11.6817\n",
      "Epoch [7/10], Training Loss: 13.4637, Training MAE: 13.4634, Test Loss: 11.0472, Test MAE: 11.0472\n",
      "Saved model with test loss: 11.0472\n",
      "Epoch [8/10], Training Loss: 13.1821, Training MAE: 13.1794, Test Loss: 10.6677, Test MAE: 10.6678\n",
      "Saved model with test loss: 10.6677\n",
      "Epoch [9/10], Training Loss: 13.0242, Training MAE: 13.0261, Test Loss: 10.3419, Test MAE: 10.3419\n",
      "Saved model with test loss: 10.3419\n",
      "Epoch [10/10], Training Loss: 12.9112, Training MAE: 12.9065, Test Loss: 10.1444, Test MAE: 10.1444\n",
      "Saved model with test loss: 10.1444\n",
      "Finished Training\n",
      "DeepCaloModelFull(\n",
      "  (upsample): Upsample(scale_factor=(1.0, 5.0), mode=nearest)\n",
      "  (conv1): Conv2d(4, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block1_conv1): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block1_conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1_2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block2_conv1): Conv2d(2, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block2_conv2): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2_2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block3_conv1): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block3_conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3_2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block4_conv1): Conv2d(12, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (block4_conv2): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4_2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=225, out_features=25, bias=True)\n",
      "  (fc2): Linear(in_features=25, out_features=25, bias=True)\n",
      "  (fc3): Linear(in_features=25, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([1, 1])\n",
      "  Iter 10/1, Params: 1835697.000000 => 17223.000000 M\n",
      "  Iter 10/1, MACs: 0.04 G => 0.00 G\n",
      "Start Training:\n",
      "Epoch [1/10], Training Loss: 19.7059, Training MAE: 19.7074, Test Loss: 13.2621, Test MAE: 13.2621\n",
      "Saved model with test loss: 13.2621\n",
      "Epoch [2/10], Training Loss: 14.6196, Training MAE: 14.6203, Test Loss: 12.1525, Test MAE: 12.1525\n",
      "Saved model with test loss: 12.1525\n",
      "Epoch [3/10], Training Loss: 13.9652, Training MAE: 13.9659, Test Loss: 11.5489, Test MAE: 11.5489\n",
      "Saved model with test loss: 11.5489\n",
      "Epoch [4/10], Training Loss: 13.6963, Training MAE: 13.6970, Test Loss: 11.0979, Test MAE: 11.0979\n",
      "Saved model with test loss: 11.0979\n",
      "Epoch [5/10], Training Loss: 13.3812, Training MAE: 13.3803, Test Loss: 10.7581, Test MAE: 10.7582\n",
      "Saved model with test loss: 10.7581\n",
      "Epoch [6/10], Training Loss: 13.1520, Training MAE: 13.1538, Test Loss: 10.6070, Test MAE: 10.6070\n",
      "Saved model with test loss: 10.6070\n",
      "Epoch [7/10], Training Loss: 13.0013, Training MAE: 12.9993, Test Loss: 10.7205, Test MAE: 10.7205\n",
      "Epoch [8/10], Training Loss: 12.7988, Training MAE: 12.8004, Test Loss: 10.3528, Test MAE: 10.3528\n",
      "Saved model with test loss: 10.3528\n",
      "Epoch [9/10], Training Loss: 12.4460, Training MAE: 12.4460, Test Loss: 10.0540, Test MAE: 10.0540\n",
      "Saved model with test loss: 10.0540\n",
      "Epoch [10/10], Training Loss: 12.3825, Training MAE: 12.3832, Test Loss: 9.9339, Test MAE: 9.9339\n",
      "Saved model with test loss: 9.9339\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "base_macs, base_nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "\n",
    "overall_list = []\n",
    "\n",
    "for i in range(10):\n",
    "    # 3. the pruner.step will remove some channels from the model with least importance\n",
    "    iterative_steps = 1 # You can prune your model to the target pruning ratio iteratively.\n",
    "    pruner = tp.pruner.MagnitudePruner(\n",
    "        model, \n",
    "        example_inputs, \n",
    "        global_pruning=False, # If False, a uniform ratio will be assigned to different layers.\n",
    "        importance=imp, # importance criterion for parameter selection\n",
    "        iterative_steps=iterative_steps, # the number of iterations to achieve target ratio\n",
    "        pruning_ratio=0.2, # remove 50% channels, ResNet18 = {64, 128, 256, 512} => ResNet18_Half = {32, 64, 128, 256}\n",
    "        ignored_layers=ignored_layers,\n",
    "    )\n",
    "\n",
    "    pruner.step()\n",
    "    \n",
    "    # 4. Do whatever you like here, such as fintuning\n",
    "    macs, nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "    print(model)\n",
    "    print(model(example_inputs).shape)\n",
    "    print(\n",
    "        \"  Iter %d/%d, Params: %.6f => %.6f M\"\n",
    "        % (i+1, iterative_steps, base_nparams, nparams )\n",
    "    )\n",
    "    print(\n",
    "        \"  Iter %d/%d, MACs: %.2f G => %.2f G\"\n",
    "        % (i+1, iterative_steps, base_macs / 1e9, macs / 1e9)\n",
    "    )\n",
    "    # finetune your model here\n",
    "    # finetune(model)\n",
    "    train_accuracies, train_losses, test_accuracies, test_losses = \\\n",
    "    train_model(model, train_loader, test_loader, num_epochs=10, lr = 0.0001, best_model_path = f'best_prune_deepalo_param_{nparams}.pth')\n",
    "    overall_list.append(test_losses)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
